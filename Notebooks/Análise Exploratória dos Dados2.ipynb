{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurações para visualização\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Carrega os dados do banco SQLite e retorna os DataFrames necessários\n",
    "    \"\"\"\n",
    "    print(\"Carregando dados do banco SQLite...\")\n",
    "    \n",
    "    # Conexão com o banco de dados\n",
    "    db_path = r'C:\\Users\\louis\\datatech\\Database\\walmart_fraudes.db'\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # Carregar tabelas como DataFrames\n",
    "    orders_df = pd.read_sql(\"SELECT * FROM orders\", conn)\n",
    "    drivers_df = pd.read_sql(\"SELECT * FROM drivers\", conn)\n",
    "    customers_df = pd.read_sql(\"SELECT * FROM customers\", conn)\n",
    "    missing_items_df = pd.read_sql(\"SELECT * FROM missing_items\", conn)\n",
    "    products_df = pd.read_sql(\"SELECT * FROM products\", conn)\n",
    "    \n",
    "    # Processamento inicial dos dados\n",
    "    orders_df['date'] = pd.to_datetime(orders_df['date'])\n",
    "    orders_df['month'] = orders_df['date'].dt.month\n",
    "    orders_df['day_of_week'] = orders_df['date'].dt.dayofweek\n",
    "    orders_df['day_name'] = orders_df['date'].dt.day_name()\n",
    "    \n",
    "    orders_df['total_items'] = orders_df['items_delivered'] + orders_df['items_missing']\n",
    "    orders_df['missing_ratio'] = orders_df['items_missing'] / orders_df['total_items']\n",
    "    \n",
    "    print(f\"Dados carregados com sucesso! Total de {len(orders_df)} pedidos.\")\n",
    "    \n",
    "    return conn, orders_df, drivers_df, customers_df, missing_items_df, products_df\n",
    "\n",
    "def exploratory_data_analysis(conn, orders_df, drivers_df, customers_df, missing_items_df, products_df):\n",
    "    \"\"\"\n",
    "    Realiza análise exploratória dos dados\n",
    "    \"\"\"\n",
    "    print(\"\\n=== INICIANDO ANÁLISE EXPLORATÓRIA DE DADOS ===\\n\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # ---------- 1. ANÁLISE UNIVARIADA ----------\n",
    "    print(\"1. Análise Univariada\")\n",
    "    \n",
    "    # Estatísticas descritivas básicas\n",
    "    print(\"\\nEstatísticas descritivas dos pedidos:\")\n",
    "    order_stats = orders_df.describe(include='all')\n",
    "    print(order_stats[['order_amount', 'items_delivered', 'items_missing', 'total_items', 'missing_ratio']])\n",
    "    \n",
    "    # Distribuição de valores dos pedidos\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(orders_df['order_amount'], kde=True, bins=30)\n",
    "    plt.title('Distribuição do Valor dos Pedidos')\n",
    "    plt.xlabel('Valor do Pedido ($)')\n",
    "    plt.ylabel('Frequência')\n",
    "    plt.savefig('figuras/distribuicao_valor_pedidos.png')\n",
    "    \n",
    "    # Distribuição de itens perdidos\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x='items_missing', data=orders_df)\n",
    "    plt.title('Distribuição de Itens Perdidos por Pedido')\n",
    "    plt.xlabel('Número de Itens Perdidos')\n",
    "    plt.ylabel('Contagem de Pedidos')\n",
    "    plt.savefig('figuras/distribuicao_itens_perdidos.png')\n",
    "    \n",
    "    # Distribuição de idade dos motoristas\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(drivers_df['age'], kde=True, bins=20)\n",
    "    plt.title('Distribuição de Idade dos Motoristas')\n",
    "    plt.xlabel('Idade')\n",
    "    plt.ylabel('Frequência')\n",
    "    plt.savefig('figuras/distribuicao_idade_motoristas.png')\n",
    "    \n",
    "    # Distribuição de idade dos clientes\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(customers_df['customer_age'], kde=True, bins=20)\n",
    "    plt.title('Distribuição de Idade dos Clientes')\n",
    "    plt.xlabel('Idade')\n",
    "    plt.ylabel('Frequência')\n",
    "    plt.savefig('figuras/distribuicao_idade_clientes.png')\n",
    "    \n",
    "    # ---------- 2. ANÁLISE BIVARIADA ----------\n",
    "    print(\"\\n2. Análise Bivariada\")\n",
    "    \n",
    "    # Relação entre valor do pedido e itens perdidos\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='items_missing', y='order_amount', data=orders_df)\n",
    "    plt.title('Relação entre Valor do Pedido e Itens Perdidos')\n",
    "    plt.xlabel('Número de Itens Perdidos')\n",
    "    plt.ylabel('Valor do Pedido ($)')\n",
    "    plt.savefig('figuras/relacao_valor_itens_perdidos.png')\n",
    "    \n",
    "    # Relação entre horário de entrega e itens perdidos\n",
    "    orders_df['delivery_hour_only'] = orders_df['delivery_hour'].apply(lambda x: int(x.split(':')[0]))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='delivery_hour_only', y='items_missing', data=orders_df)\n",
    "    plt.title('Relação entre Horário de Entrega e Itens Perdidos')\n",
    "    plt.xlabel('Hora de Entrega (hora do dia)')\n",
    "    plt.ylabel('Número de Itens Perdidos')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.savefig('figuras/relacao_hora_itens_perdidos.png')\n",
    "    \n",
    "    # Correlação entre variáveis numéricas\n",
    "    numeric_cols = ['order_amount', 'items_delivered', 'items_missing', 'total_items', 'missing_ratio', 'delivery_hour_only']\n",
    "    corr_matrix = orders_df[numeric_cols].corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Matriz de Correlação entre Variáveis')\n",
    "    plt.savefig('figuras/matriz_correlacao.png')\n",
    "    \n",
    "    # ---------- 3. ANÁLISE GEOGRÁFICA ----------\n",
    "    print(\"\\n3. Análise Geográfica\")\n",
    "    \n",
    "    # Número de pedidos por região\n",
    "    region_orders = orders_df.groupby('region').size().reset_index(name='order_count')\n",
    "    region_orders = region_orders.sort_values('order_count', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='region', y='order_count', data=region_orders)\n",
    "    plt.title('Número de Pedidos por Região')\n",
    "    plt.xlabel('Região')\n",
    "    plt.ylabel('Número de Pedidos')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig('figuras/pedidos_por_regiao.png')\n",
    "    \n",
    "    # Média de itens perdidos por região\n",
    "    region_missing = orders_df.groupby('region')['items_missing'].mean().reset_index(name='avg_missing_items')\n",
    "    region_missing = region_missing.sort_values('avg_missing_items', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='region', y='avg_missing_items', data=region_missing)\n",
    "    plt.title('Média de Itens Perdidos por Região')\n",
    "    plt.xlabel('Região')\n",
    "    plt.ylabel('Média de Itens Perdidos')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig('figuras/media_itens_perdidos_regiao.png')\n",
    "    \n",
    "    # Taxa de itens perdidos por região\n",
    "    region_stats = orders_df.groupby('region').agg({\n",
    "        'items_missing': 'sum',\n",
    "        'total_items': 'sum'\n",
    "    }).reset_index()\n",
    "    region_stats['missing_rate'] = region_stats['items_missing'] / region_stats['total_items'] * 100\n",
    "    region_stats = region_stats.sort_values('missing_rate', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='region', y='missing_rate', data=region_stats)\n",
    "    plt.title('Taxa de Itens Perdidos por Região (%)')\n",
    "    plt.xlabel('Região')\n",
    "    plt.ylabel('Taxa de Itens Perdidos (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig('figuras/taxa_itens_perdidos_regiao.png')\n",
    "    \n",
    "    results['region_stats'] = region_stats\n",
    "    \n",
    "    # ---------- 4. ANÁLISE TEMPORAL ----------\n",
    "    print(\"\\n4. Análise Temporal\")\n",
    "    \n",
    "    # Tendência ao longo do tempo\n",
    "    monthly_stats = orders_df.groupby('month').agg({\n",
    "        'order_id': 'count',\n",
    "        'items_missing': 'sum',\n",
    "        'total_items': 'sum'\n",
    "    }).reset_index()\n",
    "    monthly_stats['missing_rate'] = monthly_stats['items_missing'] / monthly_stats['total_items'] * 100\n",
    "    monthly_stats['month_name'] = monthly_stats['month'].apply(lambda x: calendar.month_name[x])\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.lineplot(x='month', y='order_id', data=monthly_stats, marker='o')\n",
    "    plt.title('Número de Pedidos por Mês')\n",
    "    plt.xlabel('Mês')\n",
    "    plt.ylabel('Número de Pedidos')\n",
    "    plt.xticks(monthly_stats['month'], rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.lineplot(x='month', y='missing_rate', data=monthly_stats, marker='o')\n",
    "    plt.title('Taxa de Itens Perdidos por Mês (%)')\n",
    "    plt.xlabel('Mês')\n",
    "    plt.ylabel('Taxa de Itens Perdidos (%)')\n",
    "    plt.xticks(monthly_stats['month'], rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figuras/tendencia_temporal.png')\n",
    "    \n",
    "    # Análise por dia da semana\n",
    "    day_stats = orders_df.groupby('day_of_week').agg({\n",
    "        'order_id': 'count',\n",
    "        'items_missing': 'sum',\n",
    "        'total_items': 'sum'\n",
    "    }).reset_index()\n",
    "    day_stats['missing_rate'] = day_stats['items_missing'] / day_stats['total_items'] * 100\n",
    "    day_stats['day_name'] = day_stats['day_of_week'].apply(lambda x: calendar.day_name[x])\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(x='day_name', y='order_id', data=day_stats)\n",
    "    plt.title('Número de Pedidos por Dia da Semana')\n",
    "    plt.xlabel('Dia da Semana')\n",
    "    plt.ylabel('Número de Pedidos')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.barplot(x='day_name', y='missing_rate', data=day_stats)\n",
    "    plt.title('Taxa de Itens Perdidos por Dia da Semana (%)')\n",
    "    plt.xlabel('Dia da Semana')\n",
    "    plt.ylabel('Taxa de Itens Perdidos (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figuras/analise_dia_semana.png')\n",
    "    \n",
    "    # Análise por hora do dia\n",
    "    hour_stats = orders_df.groupby('delivery_hour_only').agg({\n",
    "        'order_id': 'count',\n",
    "        'items_missing': 'sum',\n",
    "        'total_items': 'sum'\n",
    "    }).reset_index()\n",
    "    hour_stats['missing_rate'] = hour_stats['items_missing'] / hour_stats['total_items'] * 100\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(x='delivery_hour_only', y='order_id', data=hour_stats)\n",
    "    plt.title('Número de Pedidos por Hora do Dia')\n",
    "    plt.xlabel('Hora do Dia')\n",
    "    plt.ylabel('Número de Pedidos')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.barplot(x='delivery_hour_only', y='missing_rate', data=hour_stats)\n",
    "    plt.title('Taxa de Itens Perdidos por Hora do Dia (%)')\n",
    "    plt.xlabel('Hora do Dia')\n",
    "    plt.ylabel('Taxa de Itens Perdidos (%)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figuras/analise_hora_dia.png')\n",
    "    \n",
    "    results['monthly_stats'] = monthly_stats\n",
    "    results['day_stats'] = day_stats\n",
    "    results['hour_stats'] = hour_stats\n",
    "    \n",
    "    # ---------- 5. SEGMENTAÇÃO ----------\n",
    "    print(\"\\n5. Segmentação e Análise de Grupos\")\n",
    "    \n",
    "    # Análise por motorista\n",
    "    print(\"\\nAnalisando padrões por motorista...\")\n",
    "    driver_stats = orders_df.groupby('driver_id').agg({\n",
    "        'order_id': 'count',\n",
    "        'items_missing': 'sum',\n",
    "        'total_items': 'sum',\n",
    "        'order_amount': 'mean'\n",
    "    }).reset_index()\n",
    "    driver_stats['missing_rate'] = driver_stats['items_missing'] / driver_stats['total_items'] * 100\n",
    "    driver_stats['orders_with_missing'] = orders_df[orders_df['items_missing'] > 0].groupby('driver_id').size().reset_index(name='orders_with_missing')['orders_with_missing']\n",
    "    driver_stats['pct_orders_with_missing'] = (driver_stats['orders_with_missing'] / driver_stats['order_id']) * 100\n",
    "    \n",
    "    # Identificando motoristas com altas taxas de itens perdidos\n",
    "    high_missing_drivers = driver_stats[driver_stats['order_id'] > 5].nlargest(10, 'missing_rate')\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.barplot(x='driver_id', y='missing_rate', data=high_missing_drivers)\n",
    "    plt.title('Top 10 Motoristas com Maior Taxa de Itens Perdidos')\n",
    "    plt.xlabel('ID do Motorista')\n",
    "    plt.ylabel('Taxa de Itens Perdidos (%)')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.savefig('figuras/top_motoristas_taxa_perdidos.png')\n",
    "    \n",
    "    # Análise por cliente\n",
    "    print(\"\\nAnalisando padrões por cliente...\")\n",
    "    customer_stats = orders_df.groupby('customer_id').agg({\n",
    "        'order_id': 'count',\n",
    "        'items_missing': 'sum',\n",
    "        'total_items': 'sum',\n",
    "        'order_amount': 'mean'\n",
    "    }).reset_index()\n",
    "    customer_stats['missing_rate'] = customer_stats['items_missing'] / customer_stats['total_items'] * 100\n",
    "    customer_stats['orders_with_missing'] = orders_df[orders_df['items_missing'] > 0].groupby('customer_id').size().reset_index(name='orders_with_missing')['orders_with_missing']\n",
    "    customer_stats['pct_orders_with_missing'] = (customer_stats['orders_with_missing'] / customer_stats['order_id']) * 100\n",
    "    \n",
    "    # Identificando clientes com altas taxas de itens perdidos\n",
    "    high_missing_customers = customer_stats[customer_stats['order_id'] > 3].nlargest(10, 'missing_rate')\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.barplot(x='customer_id', y='missing_rate', data=high_missing_customers)\n",
    "    plt.title('Top 10 Clientes com Maior Taxa de Itens Perdidos')\n",
    "    plt.xlabel('ID do Cliente')\n",
    "    plt.ylabel('Taxa de Itens Perdidos (%)')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.savefig('figuras/top_clientes_taxa_perdidos.png')\n",
    "    \n",
    "    # Análise por categoria de produto\n",
    "    print(\"\\nAnalisando padrões por categoria de produto...\")\n",
    "    \n",
    "    # Juntando dados de produtos perdidos com informações do produto\n",
    "    missing_products = pd.merge(\n",
    "        missing_items_df.melt(id_vars='order_id', value_vars=['product_id_1', 'product_id_2', 'product_id_3'], \n",
    "                             value_name='product_id').dropna(),\n",
    "        products_df,\n",
    "        on='product_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Contagem de produtos perdidos por categoria\n",
    "    missing_by_category = missing_products.groupby('category').size().reset_index(name='count')\n",
    "    missing_by_category = missing_by_category.sort_values('count', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='category', y='count', data=missing_by_category)\n",
    "    plt.title('Número de Produtos Perdidos por Categoria')\n",
    "    plt.xlabel('Categoria')\n",
    "    plt.ylabel('Número de Produtos Perdidos')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig('figuras/produtos_perdidos_categoria.png')\n",
    "    \n",
    "    # Top 10 produtos mais reportados como perdidos\n",
    "    missing_by_product = missing_products.groupby(['product_id', 'product_name', 'price']).size().reset_index(name='count')\n",
    "    missing_by_product = missing_by_product.sort_values('count', ascending=False).head(10)\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.barplot(x='product_name', y='count', data=missing_by_product)\n",
    "    plt.title('Top 10 Produtos Mais Reportados como Perdidos')\n",
    "    plt.xlabel('Nome do Produto')\n",
    "    plt.ylabel('Contagem')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figuras/top_produtos_perdidos.png')\n",
    "    \n",
    "    # Análise por valor do produto\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x='price', y='count', data=missing_by_product, s=100)\n",
    "    for i, row in missing_by_product.iterrows():\n",
    "        plt.annotate(row['product_name'], (row['price'], row['count']), fontsize=8)\n",
    "    plt.title('Relação entre Preço do Produto e Frequência de Perda')\n",
    "    plt.xlabel('Preço do Produto ($)')\n",
    "    plt.ylabel('Frequência de Perda')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figuras/relacao_preco_frequencia_perda.png')\n",
    "    \n",
    "    results['driver_stats'] = driver_stats\n",
    "    results['customer_stats'] = customer_stats\n",
    "    results['missing_by_category'] = missing_by_category\n",
    "    results['missing_by_product'] = missing_by_product\n",
    "    \n",
    "    print(\"\\nAnálise exploratória concluída! Visualizações foram salvas na pasta 'figuras/'\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def detect_fraud_patterns(conn, results, orders_df):\n",
    "    \"\"\"\n",
    "    Detecta padrões de fraude nos dados\n",
    "    \"\"\"\n",
    "    print(\"\\n=== DETECÇÃO DE PADRÕES DE FRAUDE ===\\n\")\n",
    "    \n",
    "    driver_stats = results['driver_stats']\n",
    "    customer_stats = results['customer_stats']\n",
    "    \n",
    "    # Detectando motoristas suspeitos usando Isolation Forest\n",
    "    print(\"Detectando motoristas com comportamento anômalo...\")\n",
    "    \n",
    "    # Selecionando apenas motoristas com número mínimo de entregas\n",
    "    min_orders = 5\n",
    "    drivers_for_analysis = driver_stats[driver_stats['order_id'] >= min_orders].copy()\n",
    "    \n",
    "    # Preparando os dados para detecção de anomalias\n",
    "    X_drivers = drivers_for_analysis[['missing_rate', 'pct_orders_with_missing']].values\n",
    "    \n",
    "    # Aplicando Isolation Forest\n",
    "    isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    drivers_for_analysis['anomaly'] = isolation_forest.fit_predict(X_drivers)\n",
    "    \n",
    "    # Identificando motoristas suspeitos (anomalias são marcadas como -1)\n",
    "    suspicious_drivers = drivers_for_analysis[drivers_for_analysis['anomaly'] == -1].sort_values('missing_rate', ascending=False)\n",
    "    \n",
    "    print(f\"Identificados {len(suspicious_drivers)} motoristas com comportamento suspeito.\")\n",
    "    print(\"\\nTop 5 motoristas mais suspeitos:\")\n",
    "    print(suspicious_drivers[['driver_id', 'order_id', 'missing_rate', 'pct_orders_with_missing']].head())\n",
    "    \n",
    "    # Visualizando os motoristas suspeitos\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(drivers_for_analysis[drivers_for_analysis['anomaly'] == 1]['missing_rate'], \n",
    "              drivers_for_analysis[drivers_for_analysis['anomaly'] == 1]['pct_orders_with_missing'],\n",
    "              c='blue', label='Normal', alpha=0.7)\n",
    "    plt.scatter(drivers_for_analysis[drivers_for_analysis['anomaly'] == -1]['missing_rate'], \n",
    "              drivers_for_analysis[drivers_for_analysis['anomaly'] == -1]['pct_orders_with_missing'],\n",
    "              c='red', label='Suspeito', alpha=0.7)\n",
    "    plt.xlabel('Taxa de Itens Perdidos (%)')\n",
    "    plt.ylabel('% de Pedidos com Itens Perdidos')\n",
    "    plt.title('Detecção de Motoristas Suspeitos')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('figuras/motoristas_suspeitos.png')\n",
    "    \n",
    "    # Detectando clientes suspeitos usando Isolation Forest\n",
    "    print(\"\\nDetectando clientes com comportamento anômalo...\")\n",
    "    \n",
    "    # Selecionando apenas clientes com número mínimo de pedidos\n",
    "    min_orders = 3\n",
    "    customers_for_analysis = customer_stats[customer_stats['order_id'] >= min_orders].copy()\n",
    "    \n",
    "    # Preparando os dados para detecção de anomalias\n",
    "    X_customers = customers_for_analysis[['missing_rate', 'pct_orders_with_missing']].values\n",
    "    \n",
    "    # Aplicando Isolation Forest\n",
    "    isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    customers_for_analysis['anomaly'] = isolation_forest.fit_predict(X_customers)\n",
    "    \n",
    "    # Identificando clientes suspeitos (anomalias são marcadas como -1)\n",
    "    suspicious_customers = customers_for_analysis[customers_for_analysis['anomaly'] == -1].sort_values('missing_rate', ascending=False)\n",
    "    \n",
    "    print(f\"Identificados {len(suspicious_customers)} clientes com comportamento suspeito.\")\n",
    "    print(\"\\nTop 5 clientes mais suspeitos:\")\n",
    "    print(suspicious_customers[['customer_id', 'order_id', 'missing_rate', 'pct_orders_with_missing']].head())\n",
    "    \n",
    "    # Visualizando os clientes suspeitos\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(customers_for_analysis[customers_for_analysis['anomaly'] == 1]['missing_rate'], \n",
    "              customers_for_analysis[customers_for_analysis['anomaly'] == 1]['pct_orders_with_missing'],\n",
    "              c='blue', label='Normal', alpha=0.7)\n",
    "    plt.scatter(customers_for_analysis[customers_for_analysis['anomaly'] == -1]['missing_rate'], \n",
    "              customers_for_analysis[customers_for_analysis['anomaly'] == -1]['pct_orders_with_missing'],\n",
    "              c='red', label='Suspeito', alpha=0.7)\n",
    "    plt.xlabel('Taxa de Itens Perdidos (%)')\n",
    "    plt.ylabel('% de Pedidos com Itens Perdidos')\n",
    "    plt.title('Detecção de Clientes Suspeitos')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('figuras/clientes_suspeitos.png')\n",
    "    \n",
    "    # Analisando padrões por valor do pedido\n",
    "    print(\"\\nAnalisando relação entre valor do pedido e itens perdidos...\")\n",
    "    \n",
    "    # Agrupando pedidos por faixas de valor\n",
    "    orders_df['value_range'] = pd.cut(orders_df['order_amount'], \n",
    "                                     bins=[0, 100, 300, 500, 1000, 10000],\n",
    "                                     labels=['0-100', '100-300', '300-500', '500-1000', '1000+'])\n",
    "    \n",
    "    value_range_stats = orders_df.groupby('value_range').agg({\n",
    "        'order_id': 'count',\n",
    "        'items_missing': 'sum',\n",
    "        'total_items': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    value_range_stats['missing_rate'] = value_range_stats['items_missing'] / value_range_stats['total_items'] * 100\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='value_range', y='missing_rate', data=value_range_stats)\n",
    "    plt.title('Taxa de Itens Perdidos por Faixa de Valor do Pedido')\n",
    "    plt.xlabel('Faixa de Valor ($)')\n",
    "    plt.ylabel('Taxa de Itens Perdidos (%)')\n",
    "    plt.savefig('figuras/taxa_perdidos_por_valor.png')\n",
    "    \n",
    "    # Identificando padrões suspeitos específicos\n",
    "    print(\"\\nIdentificando padrões suspeitos específicos...\")\n",
    "    \n",
    "    # Padrão 1: Motoristas que têm alta taxa de perda em horários específicos\n",
    "    driver_hour_stats = orders_df.groupby(['driver_id', 'delivery_hour_only']).agg({\n",
    "        'order_id': 'count',\n",
    "        'items_missing': 'sum',\n",
    "        'total_items': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    driver_hour_stats['missing_rate'] = driver_hour_stats['items_missing'] / driver_hour_stats['total_items'] * 100\n",
    "    driver_hour_high_missing = driver_hour_stats[(driver_hour_stats['order_id'] >= 3) & \n",
    "                                               (driver_hour_stats['missing_rate'] >= 30)].sort_values('missing_rate', ascending=False)\n",
    "    \n",
    "    print(\"\\nMotoristas com alta taxa de perda em horários específicos:\")\n",
    "    print(driver_hour_high_missing[['driver_id', 'delivery_hour_only', 'order_id', 'missing_rate']].head(5))\n",
    "    \n",
    "    # Padrão 2: Motoristas com alta taxa de perda em regiões específicas\n",
    "    driver_region_stats = orders_df.groupby(['driver_id', 'region']).agg({\n",
    "        'order_id': 'count',\n",
    "        'items_missing': 'sum',\n",
    "        'total_items': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    driver_region_stats['missing_rate'] = driver_region_stats['items_missing'] / driver_region_stats['total_items'] * 100\n",
    "    driver_region_high_missing = driver_region_stats[(driver_region_stats['order_id'] >= 3) & \n",
    "                                                  (driver_region_stats['missing_rate'] >= 30)].sort_values('missing_rate', ascending=False)\n",
    "    \n",
    "    print(\"\\nMotoristas com alta taxa de perda em regiões específicas:\")\n",
    "    print(driver_region_high_missing[['driver_id', 'region', 'order_id', 'missing_rate']].head(5))\n",
    "    \n",
    "    # Padrão 3: Clientes que reportam itens perdidos em dias específicos\n",
    "    customer_day_stats = orders_df.groupby(['customer_id', 'day_name']).agg({\n",
    "        'order_id': 'count',\n",
    "        'items_missing': 'sum',\n",
    "        'total_items': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    customer_day_stats['missing_rate'] = customer_day_stats['items_missing'] / customer_day_stats['total_items'] * 100\n",
    "    customer_day_high_missing = customer_day_stats[(customer_day_stats['order_id'] >= 3) & \n",
    "                                                (customer_day_stats['missing_rate'] >= 30)].sort_values('missing_rate', ascending=False)\n",
    "    \n",
    "    print(\"\\nClientes que reportam itens perdidos em dias específicos:\")\n",
    "    print(customer_day_high_missing[['customer_id', 'day_name', 'order_id', 'missing_rate']].head(5))\n",
    "    \n",
    "    suspicious_patterns = {\n",
    "        'driver_hour_high_missing': driver_hour_high_missing,\n",
    "        'driver_region_high_missing': driver_region_high_missing,\n",
    "        'customer_day_high_missing': customer_day_high_missing\n",
    "    }\n",
    "    \n",
    "    print(\"\\nDetecção de padrões de fraude concluída!\")\n",
    "    \n",
    "    return suspicious_drivers, suspicious_customers, suspicious_patterns\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Função principal que executa todo o pipeline de análise\n",
    "    \"\"\"\n",
    "    print(\"=== PROJETO DE DETECÇÃO DE FRAUDES EM ENTREGAS DO WALMART ===\\n\")\n",
    "    \n",
    "    # Carregar os dados\n",
    "    conn, orders_df, drivers_df, customers_df, missing_items_df, products_df = load_data()\n",
    "    \n",
    "    # Realizar análise exploratória dos dados\n",
    "    results = exploratory_data_analysis(conn, orders_df, drivers_df, customers_df, missing_items_df, products_df)\n",
    "    \n",
    "    # Detectar padrões de fraude\n",
    "    suspicious_drivers, suspicious_customers, suspicious_patterns = detect_fraud_patterns(conn, results, orders_df)\n",
    "    \n",
    "    # Fechar a conexão com o banco de dados\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"\\n=== RESUMO DAS DESCOBERTAS ===\")\n",
    "    \n",
    "    # Resumo das análises por região\n",
    "    region_stats = results['region_stats'].sort_values('missing_rate', ascending=False)\n",
    "    print(\"\\nRegiões com maior taxa de itens perdidos:\")\n",
    "    for i, row in region_stats.head(3).iterrows():\n",
    "        print(f\"• {row['region']}: {row['missing_rate']:.2f}% de itens reportados como não entregues\")\n",
    "    \n",
    "    # Resumo dos motoristas suspeitos\n",
    "    print(\"\\nMotoristas com comportamento suspeito:\")\n",
    "    for i, driver in enumerate(suspicious_drivers.head(5).iterrows()):\n",
    "        driver_data = driver[1]\n",
    "        print(f\"• ID: {driver_data['driver_id']}, Nome: {driver_data['driver_name']}\")\n",
    "        print(f\"  {driver_data['missing_items_count']} itens reportados como não entregues em {driver_data['orders_with_missing']} pedidos\")\n",
    "        print(f\"  Taxa de problemas: {driver_data['problem_rate']:.2f}%\")\n",
    "    \n",
    "    # Resumo dos clientes suspeitos\n",
    "    print(\"\\nClientes com comportamento suspeito:\")\n",
    "    for i, customer in enumerate(suspicious_customers.head(5).iterrows()):\n",
    "        customer_data = customer[1]\n",
    "        print(f\"• ID: {customer_data['customer_id']}, Nome: {customer_data['customer_name']}\")\n",
    "        print(f\"  {customer_data['missing_items_count']} itens reportados como não entregues em {customer_data['orders_with_missing']} pedidos\")\n",
    "        print(f\"  Taxa de problemas: {customer_data['problem_rate']:.2f}%\")\n",
    "    \n",
    "    # Padrões de produtos mais frequentemente reportados como não entregues\n",
    "    print(\"\\nCategorias de produtos mais reportados como não entregues:\")\n",
    "    for i, product in enumerate(suspicious_patterns['product_categories'].head(3).iterrows()):\n",
    "        product_data = product[1]\n",
    "        print(f\"• {product_data['category']}: {product_data['missing_count']} ocorrências, {product_data['missing_rate']:.2f}% de taxa de não entrega\")\n",
    "    \n",
    "    # Horários com maior incidência de problemas\n",
    "    print(\"\\nHorários com maior incidência de problemas:\")\n",
    "    for i, time_slot in enumerate(suspicious_patterns['time_slots'].head(3).iterrows()):\n",
    "        time_data = time_slot[1]\n",
    "        print(f\"• {time_data['time_slot']}: {time_data['missing_rate']:.2f}% de taxa de não entrega\")\n",
    "    \n",
    "    # Criar e treinar modelo preditivo para detecção de fraudes\n",
    "    print(\"\\n=== CRIANDO MODELO PREDITIVO ===\")\n",
    "    model, train_features, test_features, train_labels, test_labels = train_fraud_detection_model(results, orders_df, drivers_df, customers_df)\n",
    "    \n",
    "    # Avaliar o modelo\n",
    "    accuracy, precision, recall, f1, auc = evaluate_model(model, test_features, test_labels)\n",
    "    \n",
    "    print(f\"\\nDesempenho do modelo:\")\n",
    "    print(f\"• Acurácia: {accuracy:.2f}\")\n",
    "    print(f\"• Precisão: {precision:.2f}\")\n",
    "    print(f\"• Recall: {recall:.2f}\")\n",
    "    print(f\"• F1-Score: {f1:.2f}\")\n",
    "    print(f\"• AUC-ROC: {auc:.2f}\")\n",
    "    \n",
    "    # Identificar características mais importantes para predição\n",
    "    features_importance = get_feature_importance(model, train_features.columns)\n",
    "    print(\"\\nCaracterísticas mais importantes para detecção de fraudes:\")\n",
    "    for feature, importance in features_importance[:5]:\n",
    "        print(f\"• {feature}: {importance:.2f}\")\n",
    "    \n",
    "    # Gerar recomendações baseadas na análise\n",
    "    print(\"\\n=== RECOMENDAÇÕES ===\")\n",
    "    recommendations = generate_recommendations(results, suspicious_drivers, suspicious_customers, suspicious_patterns)\n",
    "    \n",
    "    for i, recommendation in enumerate(recommendations[:5], 1):\n",
    "        print(f\"{i}. {recommendation}\")\n",
    "    \n",
    "    # Salvar resultados e gerar visualizações\n",
    "    print(\"\\n=== SALVANDO RESULTADOS E GERANDO VISUALIZAÇÕES ===\")\n",
    "    save_results(results, suspicious_drivers, suspicious_customers, \n",
    "                suspicious_patterns, model, accuracy, precision, recall, f1, auc)\n",
    "    \n",
    "    generate_dashboard(results, suspicious_drivers, suspicious_customers, suspicious_patterns)\n",
    "    \n",
    "    print(\"\\n=== ANÁLISE CONCLUÍDA ===\")\n",
    "    print(\"O relatório completo, dashboard e recomendações estão disponíveis nos arquivos gerados\")\n",
    "    \n",
    "    # Estimativa de redução de fraudes\n",
    "    current_fraud_loss = 400000000  # 400 milhões de dólares (aumento de 2022 para 2023)\n",
    "    estimated_reduction = calculate_fraud_reduction_estimate(results, model)\n",
    "    estimated_savings = current_fraud_loss * estimated_reduction / 100\n",
    "    \n",
    "    print(f\"\\nEstimativa de redução de fraudes com as medidas sugeridas: {estimated_reduction:.1f}%\")\n",
    "    print(f\"Economia potencial anual: ${estimated_savings/1000000:.1f} milhões\")\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Carrega os dados do banco SQLite e retorna os DataFrames\n",
    "    \"\"\"\n",
    "    print(\"Carregando dados...\")\n",
    "    \n",
    "    # Caminho para o banco de dados SQLite\n",
    "    db_path = r'C:\\Users\\louis\\datatech\\Database\\walmart_fraudes.db'\n",
    "    \n",
    "    try:\n",
    "        # Conectando ao banco\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        # Carregando os dados em DataFrames\n",
    "        orders_df = pd.read_sql(\"SELECT * FROM orders\", conn)\n",
    "        drivers_df = pd.read_sql(\"SELECT * FROM drivers\", conn)\n",
    "        customers_df = pd.read_sql(\"SELECT * FROM customers\", conn)\n",
    "        missing_items_df = pd.read_sql(\"SELECT * FROM missing_items\", conn)\n",
    "        products_df = pd.read_sql(\"SELECT * FROM products\", conn)\n",
    "        \n",
    "        # Formatando os dados\n",
    "        orders_df['date'] = pd.to_datetime(orders_df['date'])\n",
    "        \n",
    "        # Verificação básica da carga de dados\n",
    "        print(f\"Dados carregados com sucesso:\")\n",
    "        print(f\"• Pedidos: {len(orders_df)} registros\")\n",
    "        print(f\"• Motoristas: {len(drivers_df)} registros\")\n",
    "        print(f\"• Clientes: {len(customers_df)} registros\")\n",
    "        print(f\"• Produtos não entregues: {len(missing_items_df)} registros\")\n",
    "        print(f\"• Produtos: {len(products_df)} registros\")\n",
    "        \n",
    "        return conn, orders_df, drivers_df, customers_df, missing_items_df, products_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar os dados: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "def exploratory_data_analysis(conn, orders_df, drivers_df, customers_df, missing_items_df, products_df):\n",
    "    \"\"\"\n",
    "    Realiza análise exploratória dos dados e retorna os resultados\n",
    "    \"\"\"\n",
    "    print(\"\\nRealizando análise exploratória dos dados...\")\n",
    "    results = {}\n",
    "    \n",
    "    # Estatísticas gerais\n",
    "    total_orders = len(orders_df)\n",
    "    total_items_delivered = orders_df['items_delivered'].sum()\n",
    "    total_items_missing = orders_df['items_missing'].sum()\n",
    "    missing_rate = (total_items_missing / (total_items_delivered + total_items_missing)) * 100\n",
    "    \n",
    "    orders_with_missing = orders_df[orders_df['items_missing'] > 0].shape[0]\n",
    "    orders_with_missing_rate = (orders_with_missing / total_orders) * 100\n",
    "    \n",
    "    print(f\"• Total de pedidos: {total_orders}\")\n",
    "    print(f\"• Pedidos com itens faltantes: {orders_with_missing} ({orders_with_missing_rate:.2f}%)\")\n",
    "    print(f\"• Taxa média de itens não entregues: {missing_rate:.2f}%\")\n",
    "    \n",
    "    results['general_stats'] = {\n",
    "        'total_orders': total_orders,\n",
    "        'total_items_delivered': total_items_delivered,\n",
    "        'total_items_missing': total_items_missing,\n",
    "        'missing_rate': missing_rate,\n",
    "        'orders_with_missing': orders_with_missing,\n",
    "        'orders_with_missing_rate': orders_with_missing_rate\n",
    "    }\n",
    "    \n",
    "    # Análise por região\n",
    "    region_stats = orders_df.groupby('region').agg(\n",
    "        total_orders=('order_id', 'count'),\n",
    "        items_delivered=('items_delivered', 'sum'),\n",
    "        items_missing=('items_missing', 'sum'),\n",
    "        orders_with_missing=('items_missing', lambda x: (x > 0).sum())\n",
    "    ).reset_index()\n",
    "    \n",
    "    region_stats['missing_rate'] = (region_stats['items_missing'] / \n",
    "                                   (region_stats['items_delivered'] + region_stats['items_missing'])) * 100\n",
    "    region_stats['problem_order_rate'] = (region_stats['orders_with_missing'] / region_stats['total_orders']) * 100\n",
    "    \n",
    "    results['region_stats'] = region_stats\n",
    "    \n",
    "    # Análise por motorista\n",
    "    driver_stats = pd.read_sql(\"\"\"\n",
    "        SELECT \n",
    "            d.driver_id,\n",
    "            d.driver_name,\n",
    "            d.age,\n",
    "            COUNT(o.order_id) AS orders_delivered,\n",
    "            SUM(o.items_missing) AS missing_items_count,\n",
    "            SUM(o.items_delivered) AS delivered_items_count,\n",
    "            SUM(CASE WHEN o.items_missing > 0 THEN 1 ELSE 0 END) AS orders_with_missing\n",
    "        FROM \n",
    "            drivers d\n",
    "        JOIN \n",
    "            orders o ON d.driver_id = o.driver_id\n",
    "        GROUP BY \n",
    "            d.driver_id\n",
    "    \"\"\", conn)\n",
    "    \n",
    "    driver_stats['missing_rate'] = (driver_stats['missing_items_count'] / \n",
    "                                   (driver_stats['delivered_items_count'] + driver_stats['missing_items_count'])) * 100\n",
    "    driver_stats['problem_rate'] = (driver_stats['orders_with_missing'] / driver_stats['orders_delivered']) * 100\n",
    "    \n",
    "    results['driver_stats'] = driver_stats\n",
    "    \n",
    "    # Análise por cliente\n",
    "    customer_stats = pd.read_sql(\"\"\"\n",
    "        SELECT \n",
    "            c.customer_id,\n",
    "            c.customer_name,\n",
    "            c.customer_age,\n",
    "            COUNT(o.order_id) AS orders_placed,\n",
    "            SUM(o.items_missing) AS missing_items_count,\n",
    "            SUM(o.items_delivered) AS delivered_items_count,\n",
    "            SUM(CASE WHEN o.items_missing > 0 THEN 1 ELSE 0 END) AS orders_with_missing\n",
    "        FROM \n",
    "            customers c\n",
    "        JOIN \n",
    "            orders o ON c.customer_id = o.customer_id\n",
    "        GROUP BY \n",
    "            c.customer_id\n",
    "    \"\"\", conn)\n",
    "    \n",
    "    customer_stats['missing_rate'] = (customer_stats['missing_items_count'] / \n",
    "                                     (customer_stats['delivered_items_count'] + customer_stats['missing_items_count'])) * 100\n",
    "    customer_stats['problem_rate'] = (customer_stats['orders_with_missing'] / customer_stats['orders_placed']) * 100\n",
    "    \n",
    "    results['customer_stats'] = customer_stats\n",
    "    \n",
    "    # Análise por hora do dia\n",
    "    orders_df['hour_of_day'] = orders_df['delivery_hour_only']\n",
    "    \n",
    "    time_stats = orders_df.groupby('hour_of_day').agg(\n",
    "        total_orders=('order_id', 'count'),\n",
    "        items_delivered=('items_delivered', 'sum'),\n",
    "        items_missing=('items_missing', 'sum'),\n",
    "        orders_with_missing=('items_missing', lambda x: (x > 0).sum())\n",
    "    ).reset_index()\n",
    "    \n",
    "    time_stats['missing_rate'] = (time_stats['items_missing'] / \n",
    "                                 (time_stats['items_delivered'] + time_stats['items_missing'])) * 100\n",
    "    time_stats['problem_order_rate'] = (time_stats['orders_with_missing'] / time_stats['total_orders']) * 100\n",
    "    \n",
    "    results['time_stats'] = time_stats\n",
    "    \n",
    "    # Análise de produtos mais frequentemente reportados como não entregues\n",
    "    # Unindo missing_items com products para obter categorias e preços\n",
    "    missing_products_stats = pd.DataFrame()\n",
    "    \n",
    "    for col in ['product_id_1', 'product_id_2', 'product_id_3']:\n",
    "        if col in missing_items_df.columns:\n",
    "            temp_df = missing_items_df[missing_items_df[col].notnull()]\n",
    "            temp_df = temp_df.rename(columns={col: 'product_id'})\n",
    "            temp_df = temp_df[['product_id']]\n",
    "            if missing_products_stats.empty:\n",
    "                missing_products_stats = temp_df\n",
    "            else:\n",
    "                missing_products_stats = pd.concat([missing_products_stats, temp_df])\n",
    "    \n",
    "    missing_products_stats = missing_products_stats.merge(\n",
    "        products_df[['product_id', 'product_name', 'category', 'price']], \n",
    "        on='product_id', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    product_category_stats = missing_products_stats.groupby('category').agg(\n",
    "        missing_count=('product_id', 'count'),\n",
    "        avg_price=('price', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calcular a taxa de não entrega por categoria\n",
    "    # Precisamos do total de produtos de cada categoria entregues\n",
    "    # Como aproximação, vamos usar a proporção de produtos na tabela de produtos\n",
    "    category_counts = products_df['category'].value_counts().reset_index()\n",
    "    category_counts.columns = ['category', 'total_products']\n",
    "    \n",
    "    product_category_stats = product_category_stats.merge(category_counts, on='category', how='left')\n",
    "    product_category_stats['missing_rate'] = (product_category_stats['missing_count'] / \n",
    "                                             product_category_stats['total_products']) * 100\n",
    "    \n",
    "    results['product_category_stats'] = product_category_stats\n",
    "    \n",
    "    # Análise por valor do pedido\n",
    "    orders_df['order_amount_range'] = pd.cut(\n",
    "        orders_df['order_amount'], \n",
    "        bins=[0, 100, 250, 500, 1000, float('inf')],\n",
    "        labels=['$0-$100', '$100-$250', '$250-$500', '$500-$1000', '$1000+']\n",
    "    )\n",
    "    \n",
    "    amount_stats = orders_df.groupby('order_amount_range').agg(\n",
    "        total_orders=('order_id', 'count'),\n",
    "        items_delivered=('items_delivered', 'sum'),\n",
    "        items_missing=('items_missing', 'sum'),\n",
    "        orders_with_missing=('items_missing', lambda x: (x > 0).sum())\n",
    "    ).reset_index()\n",
    "    \n",
    "    amount_stats['missing_rate'] = (amount_stats['items_missing'] / \n",
    "                                   (amount_stats['items_delivered'] + amount_stats['items_missing'])) * 100\n",
    "    amount_stats['problem_order_rate'] = (amount_stats['orders_with_missing'] / amount_stats['total_orders']) * 100\n",
    "    \n",
    "    results['amount_stats'] = amount_stats\n",
    "    \n",
    "    # Análise por período do dia\n",
    "    period_stats = orders_df.groupby('period_of_day').agg(\n",
    "        total_orders=('order_id', 'count'),\n",
    "        items_delivered=('items_delivered', 'sum'),\n",
    "        items_missing=('items_missing', 'sum'),\n",
    "        orders_with_missing=('items_missing', lambda x: (x > 0).sum())\n",
    "    ).reset_index()\n",
    "    \n",
    "    period_stats['missing_rate'] = (period_stats['items_missing'] / \n",
    "                                  (period_stats['items_delivered'] + period_stats['items_missing'])) * 100\n",
    "    period_stats['problem_order_rate'] = (period_stats['orders_with_missing'] / period_stats['total_orders']) * 100\n",
    "    \n",
    "    results['period_stats'] = period_stats\n",
    "    \n",
    "    print(\"Análise exploratória concluída com sucesso!\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def detect_fraud_patterns(conn, results, orders_df):\n",
    "    \"\"\"\n",
    "    Detecta padrões de fraude nos dados analisados\n",
    "    \"\"\"\n",
    "    print(\"\\nDetectando padrões de fraude...\")\n",
    "    \n",
    "    # Identificar motoristas suspeitos\n",
    "    driver_stats = results['driver_stats']\n",
    "    overall_missing_rate = results['general_stats']['missing_rate']\n",
    "    overall_problem_rate = results['general_stats']['orders_with_missing_rate']\n",
    "    \n",
    "    # Definindo limiares para detecção de motoristas suspeitos\n",
    "    # Motoristas com taxa de itens faltantes muito acima da média\n",
    "    missing_rate_threshold = overall_missing_rate * 1.5\n",
    "    problem_rate_threshold = overall_problem_rate * 1.5\n",
    "    \n",
    "    suspicious_drivers = driver_stats[\n",
    "        (driver_stats['missing_rate'] > missing_rate_threshold) &\n",
    "        (driver_stats['problem_rate'] > problem_rate_threshold) &\n",
    "        (driver_stats['orders_delivered'] > 5)  # Pelo menos 5 entregas para ser estatisticamente relevante\n",
    "    ].sort_values('problem_rate', ascending=False)\n",
    "    \n",
    "    print(f\"• Identificados {len(suspicious_drivers)} motoristas com comportamento suspeito\")\n",
    "    \n",
    "    # Identificar clientes suspeitos\n",
    "    customer_stats = results['customer_stats']\n",
    "    \n",
    "    suspicious_customers = customer_stats[\n",
    "        (customer_stats['missing_rate'] > missing_rate_threshold) &\n",
    "        (customer_stats['problem_rate'] > problem_rate_threshold) &\n",
    "        (customer_stats['orders_placed'] > 5)  # Pelo menos 5 pedidos para ser estatisticamente relevante\n",
    "    ].sort_values('problem_rate', ascending=False)\n",
    "    \n",
    "    print(f\"• Identificados {len(suspicious_customers)} clientes com comportamento suspeito\")\n",
    "    \n",
    "    # Identificar padrões de tempo/região/produto\n",
    "    suspicious_patterns = {}\n",
    "    \n",
    "    # Horários mais problemáticos\n",
    "    time_slots = results['time_stats'].copy()\n",
    "    time_slots.columns = ['time_slot' if col == 'hour_of_day' else col for col in time_slots.columns]\n",
    "    suspicious_patterns['time_slots'] = time_slots[\n",
    "        (time_slots['missing_rate'] > overall_missing_rate * 1.2) &\n",
    "        (time_slots['total_orders'] > 50)  # Número mínimo de pedidos para ser estatisticamente relevante\n",
    "    ].sort_values('missing_rate', ascending=False)\n",
    "    \n",
    "    # Categorias de produtos mais frequentemente relatadas como não entregues\n",
    "    suspicious_patterns['product_categories'] = results['product_category_stats'].sort_values('missing_rate', ascending=False)\n",
    "    \n",
    "    # Regiões com maior incidência de problemas\n",
    "    suspicious_patterns['regions'] = results['region_stats'][\n",
    "        (results['region_stats']['missing_rate'] > overall_missing_rate * 1.2) &\n",
    "        (results['region_stats']['total_orders'] > 50)  # Número mínimo de pedidos para ser estatisticamente relevante\n",
    "    ].sort_values('missing_rate', ascending=False)\n",
    "    \n",
    "    # Padrões de valor de pedidos\n",
    "    suspicious_patterns['order_amounts'] = results['amount_stats'][\n",
    "        results['amount_stats']['missing_rate'] > overall_missing_rate * 1.2\n",
    "    ].sort_values('missing_rate', ascending=False)\n",
    "    \n",
    "    # Verificar combinações específicas (motorista-região, motorista-horário, etc.)\n",
    "    driver_region_stats = pd.read_sql(\"\"\"\n",
    "        SELECT \n",
    "            d.driver_id,\n",
    "            o.region,\n",
    "            COUNT(o.order_id) AS total_orders,\n",
    "            SUM(o.items_missing) AS items_missing,\n",
    "            SUM(o.items_delivered) AS items_delivered,\n",
    "            SUM(CASE WHEN o.items_missing > 0 THEN 1 ELSE 0 END) AS orders_with_missing\n",
    "        FROM \n",
    "            drivers d\n",
    "        JOIN \n",
    "            orders o ON d.driver_id = o.driver_id\n",
    "        GROUP BY \n",
    "            d.driver_id, o.region\n",
    "        HAVING \n",
    "            total_orders > 3\n",
    "    \"\"\", conn)\n",
    "    \n",
    "    driver_region_stats['missing_rate'] = (driver_region_stats['items_missing'] / \n",
    "                                         (driver_region_stats['items_delivered'] + driver_region_stats['items_missing'])) * 100\n",
    "    driver_region_stats['problem_rate'] = (driver_region_stats['orders_with_missing'] / driver_region_stats['total_orders']) * 100\n",
    "    \n",
    "    suspicious_patterns['driver_region'] = driver_region_stats[\n",
    "        (driver_region_stats['missing_rate'] > missing_rate_threshold) &\n",
    "        (driver_region_stats['problem_rate'] > problem_rate_threshold)\n",
    "    ].sort_values('problem_rate', ascending=False)\n",
    "    \n",
    "    # Encontrar padrões de produtos específicos por motorista\n",
    "    # Esta análise exigiria unir várias tabelas e ficaria demasiado complexa neste código de exemplo\n",
    "    \n",
    "    print(\"Detecção de padrões de fraude concluída!\")\n",
    "    return suspicious_drivers, suspicious_customers, suspicious_patterns\n",
    "\n",
    "\n",
    "def train_fraud_detection_model(results, orders_df, drivers_df, customers_df):\n",
    "    \"\"\"\n",
    "    Treina um modelo de machine learning para detectar possíveis fraudes\n",
    "    \"\"\"\n",
    "    print(\"Treinando modelo de detecção de fraudes...\")\n",
    "    \n",
    "    # Preparar conjunto de dados para treino\n",
    "    # Unir dados de pedidos com estatísticas dos motoristas e clientes\n",
    "    model_data = orders_df.copy()\n",
    "    \n",
    "    # Adicionar recursos do motorista\n",
    "    driver_features = results['driver_stats'][['driver_id', 'age', 'missing_rate', 'problem_rate']].rename(\n",
    "        columns={'missing_rate': 'driver_missing_rate', 'problem_rate': 'driver_problem_rate'}\n",
    "    )\n",
    "    model_data = model_data.merge(driver_features, on='driver_id', how='left')\n",
    "    \n",
    "    # Adicionar recursos do cliente\n",
    "    customer_features = results['customer_stats'][['customer_id', 'customer_age', 'missing_rate', 'problem_rate']].rename(\n",
    "        columns={'missing_rate': 'customer_missing_rate', 'problem_rate': 'customer_problem_rate'}\n",
    "    )\n",
    "    model_data = model_data.merge(customer_features, on='customer_id', how='left')\n",
    "    \n",
    "    # Adicionar recursos da região\n",
    "    region_features = results['region_stats'][['region', 'missing_rate', 'problem_order_rate']].rename(\n",
    "        columns={'missing_rate': 'region_missing_rate', 'problem_order_rate': 'region_problem_rate'}\n",
    "    )\n",
    "    model_data = model_data.merge(region_features, on='region', how='left')\n",
    "    \n",
    "    # Criar variável alvo\n",
    "    # Definimos como \"suspeito\" um pedido onde:\n",
    "    # - Mais de 20% dos itens foram relatados como não entregues\n",
    "    # - O motorista tem alta taxa de problemas\n",
    "    # - O cliente tem alta taxa de problemas \n",
    "    # - A taxa de itens não entregues é maior que a média da região\n",
    "    \n",
    "    model_data['missing_rate'] = (model_data['items_missing'] / \n",
    "                                 (model_data['items_delivered'] + model_data['items_missing'])) * 100\n",
    "    \n",
    "    # Definir limiar para classificar um pedido como suspeito\n",
    "    overall_missing_rate = results['general_stats']['missing_rate']\n",
    "    \n",
    "    model_data['suspicious'] = (\n",
    "        (model_data['missing_rate'] > 20) | \n",
    "        ((model_data['driver_problem_rate'] > results['general_stats']['orders_with_missing_rate'] * 1.5) & \n",
    "         (model_data['missing_rate'] > overall_missing_rate)) |\n",
    "        ((model_data['customer_problem_rate'] > results['general_stats']['orders_with_missing_rate'] * 1.5) & \n",
    "         (model_data['missing_rate'] > overall_missing_rate)) |\n",
    "        ((model_data['missing_rate'] > model_data['region_missing_rate'] * 1.5) & \n",
    "         (model_data['missing_rate'] > 10))\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Criar recursos (features) para o modelo\n",
    "    # Transformar variáveis categóricas\n",
    "    model_data['hour'] = model_data['delivery_hour_only']\n",
    "    model_data['month'] = pd.to_datetime(model_data['date']).dt.month\n",
    "    model_data['day_of_week'] = pd.to_datetime(model_data['date']).dt.dayofweek\n",
    "    \n",
    "    # Codificar variáveis categóricas\n",
    "    model_data = pd.get_dummies(model_data, columns=['region', 'period_of_day'], drop_first=True)\n",
    "    \n",
    "    # Selecionar recursos para o modelo\n",
    "    features = [\n",
    "        'order_amount', 'items_delivered', 'items_missing', 'hour', \n",
    "        'month', 'day_of_week', 'age', 'driver_missing_rate', 'driver_problem_rate',\n",
    "        'customer_age', 'customer_missing_rate', 'customer_problem_rate', \n",
    "        'region_missing_rate', 'region_problem_rate'\n",
    "    ] + [col for col in model_data.columns if col.startswith('region_') or col.startswith('period_of_day_')]\n",
    "    \n",
    "    # Remover linhas com valores ausentes\n",
    "    model_data = model_data.dropna(subset=features + ['suspicious'])\n",
    "    \n",
    "    # Dividir os dados em treino e teste\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X = model_data[features]\n",
    "    y = model_data['suspicious']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Treinar modelo\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=10,\n",
    "        min_samples_split=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Modelo treinado com {len(X_train)} amostras.\")\n",
    "    return model, X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Avalia o desempenho do modelo de detecção de fraudes\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "    \n",
    "    # Fazer previsões\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, auc\n",
    "\n",
    "\n",
    "def get_feature_importance(model, feature_names):\n",
    "    \"\"\"\n",
    "    Retorna a importância das características no modelo\n",
    "    \"\"\"\n",
    "    importances = model.feature_importances_\n",
    "    feature_importance = list(zip(feature_names, importances))\n",
    "    return sorted(feature_importance, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def generate_recommendations(results, suspicious_drivers, suspicious_customers, suspicious_patterns):\n",
    "    \"\"\"\n",
    "    Gera recomendações baseadas na análise dos dados\n",
    "    \"\"\"\n",
    "    recommendations = [\n",
    "        \"Implementar um sistema de verificação fotográfica para entregas de alto valor.\",\n",
    "        \"Realizar auditorias periódicas nos motoristas com alta taxa de itens relatados como não entregues.\",\n",
    "        \"Implementar um sistema de assinatura digital para confirmação de entrega completa.\",\n",
    "        \"Estabelecer um programa de treinamento e conscientização para motoristas sobre a importância da entrega completa.\",\n",
    "        \"Implementar um sistema de geolocalização para verificar se o motorista esteve no endereço de entrega pelo tempo adequado.\",\n",
    "        \"Criar um sistema de avaliação cruzada para clientes que reportam itens não entregues com frequência.\",\n",
    "        \"Implementar um sistema de alertas para pedidos com produtos de categorias frequentemente relatadas como não entregues.\",\n",
    "        \"Estabelecer um programa de incentivos para motoristas com baixas taxas de problemas relatados.\",\n",
    "        \"Desenvolver um algoritmo de priorização para investigações de casos de não entrega baseado no valor do item e no histórico do motorista e cliente.\",\n",
    "        \"Implementar embalagens especiais com selo de segurança para produtos de maior valor.\",\n",
    "        \"Realizar pesquisas de satisfação direcionadas após cada entrega para avaliar a qualidade do serviço.\",\n",
    "        \"Atribuir motoristas mais experientes para entregas em regiões com alta taxa de problemas.\"\n",
    "    ]\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "\n",
    "def save_results(results, suspicious_drivers, suspicious_customers, suspicious_patterns, model, accuracy, precision, recall, f1, auc):\n",
    "    \"\"\"\n",
    "    Salva os resultados da análise em arquivos para referência futura\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Criar diretório para resultados se não existir\n",
    "    os.makedirs(\"resultados\", exist_ok=True)\n",
    "    \n",
    "    # Salvar estatísticas gerais\n",
    "    with open(\"resultados/estatisticas_gerais.txt\", \"w\") as f:\n",
    "        f.write(\"ESTATÍSTICAS GERAIS\\n\")\n",
    "        f.write(\"===================\\n\\n\")\n",
    "        f.write(f\"Total de pedidos: {results['general_stats']['total_orders']}\\n\")\n",
    "        f.write(f\"Total de itens entregues: {results['general_stats']['total_items_delivered']}\\n\")\n",
    "        f.write(f\"Total de itens não entregues: {results['general_stats']['total_items_not_delivered']}\\n\")\n",
    "        f.write(f\"Taxa de não entrega: {results['general_stats']['non_delivery_rate']:.2f}%\\n\")\n",
    "        f.write(f\"Valor total de pedidos: R$ {results['general_stats']['total_order_value']:.2f}\\n\")\n",
    "        f.write(f\"Valor médio por pedido: R$ {results['general_stats']['avg_order_value']:.2f}\\n\\n\")\n",
    "    \n",
    "    # Salvar motoristas suspeitos\n",
    "    with open(\"resultados/motoristas_suspeitos.csv\", \"w\") as f:\n",
    "        f.write(\"id_motorista,nome,total_pedidos,taxa_nao_entrega,score_suspeita\\n\")\n",
    "        for driver in suspicious_drivers:\n",
    "            f.write(f\"{driver['id']},{driver['name']},{driver['total_orders']},{driver['non_delivery_rate']:.2f},{driver['suspicion_score']:.2f}\\n\")\n",
    "    \n",
    "    # Salvar clientes suspeitos\n",
    "    with open(\"resultados/clientes_suspeitos.csv\", \"w\") as f:\n",
    "        f.write(\"id_cliente,nome,total_pedidos,taxa_reclamacao,score_suspeita\\n\")\n",
    "        for customer in suspicious_customers:\n",
    "            f.write(f\"{customer['id']},{customer['name']},{customer['total_orders']},{customer['complaint_rate']:.2f},{customer['suspicion_score']:.2f}\\n\")\n",
    "    \n",
    "    # Salvar padrões suspeitos\n",
    "    with open(\"resultados/padroes_suspeitos.txt\", \"w\") as f:\n",
    "        f.write(\"PADRÕES SUSPEITOS IDENTIFICADOS\\n\")\n",
    "        f.write(\"==============================\\n\\n\")\n",
    "        for i, pattern in enumerate(suspicious_patterns, 1):\n",
    "            f.write(f\"Padrão {i}:\\n\")\n",
    "            f.write(f\"Descrição: {pattern['description']}\\n\")\n",
    "            f.write(f\"Ocorrências: {pattern['occurrences']}\\n\")\n",
    "            f.write(f\"Confiança: {pattern['confidence']:.2f}\\n\")\n",
    "            f.write(f\"Impacto estimado: R$ {pattern['estimated_impact']:.2f}\\n\\n\")\n",
    "    \n",
    "    # Salvar métricas do modelo\n",
    "    with open(\"resultados/metricas_modelo.txt\", \"w\") as f:\n",
    "        f.write(\"MÉTRICAS DO MODELO DE DETECÇÃO\\n\")\n",
    "        f.write(\"=============================\\n\\n\")\n",
    "        f.write(f\"Modelo utilizado: {model}\\n\")\n",
    "        f.write(f\"Acurácia: {accuracy:.4f}\\n\")\n",
    "        f.write(f\"Precisão: {precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.4f}\\n\")\n",
    "        f.write(f\"F1-Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"AUC-ROC: {auc:.4f}\\n\")\n",
    "    \n",
    "    # Opcional: exportar dados completos em formato JSON para análise futura\n",
    "    import json\n",
    "    with open(\"resultados/dados_completos.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    print(\"Resultados salvos com sucesso no diretório 'resultados/'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
